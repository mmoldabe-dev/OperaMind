import os
import json
import math
import re
import speech_recognition as sr
from pydub import AudioSegment
from pathlib import Path

try:
    from vosk import Model as VoskModel, KaldiRecognizer
    _VOSK_AVAILABLE = True
except Exception:
    _VOSK_AVAILABLE = False

# –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
try:
    import numpy as np
    import librosa
    from sklearn.cluster import KMeans
    _CLUSTER_AVAILABLE = True
except Exception:
    _CLUSTER_AVAILABLE = False


def _split_sentences(text: str):
    # –ü—Ä–æ—Å—Ç–µ–π—à–µ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
    raw = re.split(r'([.!?])', text)
    sentences = []
    current = ''
    for part in raw:
        if not part:
            continue
        current += part
        if part in '.!?':
            cleaned = current.strip()
            if cleaned:
                sentences.append(cleaned)
            current = ''
    # –•–≤–æ—Å—Ç
    tail = current.strip()
    if tail:
        sentences.append(tail)
    return [s for s in (seg.strip() for seg in sentences) if s]


def _approx_words(sentence_text: str, start_time: float, end_time: float):
    words = [w for w in re.split(r'\s+', sentence_text.strip()) if w]
    if not words:
        return []
    duration = max(end_time - start_time, 0.0001)
    per = duration / len(words)
    out = []
    t = start_time
    for w in words:
        out.append({
            'w': w,
            'start': round(t, 3),
            'end': round(t + per, 3)
        })
        t += per
    # –ö–æ—Ä—Ä–µ–∫—Ü–∏—è –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ
    if out:
        out[-1]['end'] = round(end_time, 3)
    return out


def transcribe_audio_file(filepath: str):
    """–†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è —Å –ø–æ–ø—ã—Ç–∫–æ–π Vosk.

    –ê–ª–≥–æ—Ä–∏—Ç–º:
      1. –ü—ã—Ç–∞–µ–º—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Vosk (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω –∏ –µ—Å—Ç—å –º–æ–¥–µ–ª—å VOSK_MODEL_PATH).
      2. –ï—Å–ª–∏ Vosk –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω ‚Äî fallback –Ω–∞ Google SpeechRecognition (—Ü–µ–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç + —ç–≤—Ä–∏—Å—Ç–∏–∫–∞).
    """
    print(f"üì§ –ó–∞–≥—Ä—É–∂–∞—é: {filepath}")
    audio = AudioSegment.from_file(filepath)
    original_ms = len(audio)
    audio_proc = audio.set_frame_rate(16000).set_channels(1)
    duration_min = original_ms / 60000
    print(f"‚è±Ô∏è –î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {duration_min:.1f} –º–∏–Ω")

    # --- –ü–æ–ø—ã—Ç–∫–∞ Vosk ---
    model_path = os.getenv('VOSK_MODEL_PATH')
    if _VOSK_AVAILABLE and model_path and Path(model_path).exists():
        try:
            print("üîä –ò—Å–ø–æ–ª—å–∑—É—é Vosk –º–æ–¥–µ–ª—å")
            temp_wav = "temp_vosk.wav"
            audio_proc.export(temp_wav, format="wav")

            from wave import open as wave_open
            import wave, json as _json
            wf = wave_open(temp_wav, "rb")
            rec = KaldiRecognizer(VoskModel(model_path), wf.getframerate())
            rec.SetWords(True)

            import json as _json2
            results = []
            while True:
                data = wf.readframes(4000)
                if len(data) == 0:
                    break
                if rec.AcceptWaveform(data):
                    results.append(_json2.loads(rec.Result()))
            results.append(_json2.loads(rec.FinalResult()))
            wf.close()
            os.remove(temp_wav)

            # –°–æ–±–∏—Ä–∞–µ–º –í–°–ï —Å–ª–æ–≤–∞ –≤ –æ–¥–∏–Ω —Å–ø–∏—Å–æ–∫, –Ω–µ —Ç–µ—Ä—è—è filler words
            raw_words = []  # [{'word':, 'start':, 'end':}
            for res in results:
                if isinstance(res, dict):
                    for w in res.get('result', []) or []:
                        if 'word' in w:
                            raw_words.append({
                                'w': w.get('word'),
                                'start': float(w.get('start', 0.0)),
                                'end': float(w.get('end', 0.0))
                            })

            if not raw_words:
                raise RuntimeError("–ü—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç Vosk")

            # –°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –ø–æ –ø–∞—É–∑–∞–º > 0.8s
            pause_threshold = float(os.getenv('PAUSE_THRESHOLD', '0.8'))
            grouped = []  # —Å–ø–∏—Å–æ–∫ —Å–µ–≥–º–µ–Ω—Ç–æ–≤ —Å –ø–æ–ª—è–º–∏ words, start, end
            current = [raw_words[0]]
            for prev, cur in zip(raw_words, raw_words[1:]):
                gap = cur['start'] - prev['end']
                if gap > pause_threshold:
                    grouped.append(current)
                    current = [cur]
                else:
                    current.append(cur)
            if current:
                grouped.append(current)

            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤ —Å–µ–≥–º–µ–Ω—Ç–æ–≤
            segments = []
            full_words_flat = []
            for idx, g in enumerate(grouped):
                start_t = g[0]['start']
                end_t = g[-1]['end']
                text_seg = ' '.join(w['w'] for w in g)
                full_words_flat.extend(w['w'] for w in g)
                segments.append({
                    'index': idx,
                    'speaker': 'unknown',  # –≤—Ä–µ–º–µ–Ω–Ω–æ, –æ–ø—Ä–µ–¥–µ–ª–∏–º –ø–æ–∑–∂–µ
                    'start': round(start_t, 3),
                    'end': round(end_t, 3),
                    'text': text_seg,
                    'words': [
                        {
                            'w': w['w'],
                            'start': round(w['start'], 3),
                            'end': round(w['end'], 3)
                        } for w in g
                    ],
                    'gap_from_prev': 0.0 if idx == 0 else round(start_t - segments[-1]['end'], 3)
                })

            # –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —Å–ø–∏–∫–µ—Ä–æ–≤ (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ –∏ –¥–æ—Å—Ç—É–ø–Ω—ã –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏)
            clustering_enabled = os.getenv('SPEAKER_CLUSTERING', '1') == '1'
            clustering_info = {}
            if clustering_enabled and _CLUSTER_AVAILABLE:
                try:
                    print("üß™ –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —Å–ø–∏–∫–µ—Ä–æ–≤ (MFCC + KMeans)...")
                    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∞—É–¥–∏–æ –æ–¥–Ω–æ–π –≤–æ–ª–Ω–æ–π –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
                    # –í–ê–ñ–ù–û: sample_rate, —á—Ç–æ–±—ã –Ω–µ –ø–µ—Ä–µ–∫—Ä—ã—Ç—å –º–æ–¥—É–ª—å speech_recognition (sr)
                    y, sample_rate = librosa.load(filepath, sr=16000, mono=True)
                    feat_vectors = []
                    for seg in segments:
                        s = max(int(seg['start'] * sample_rate), 0)
                        e = min(int(seg['end'] * sample_rate), len(y))
                        chunk = y[s:e]
                        if len(chunk) < sample_rate * 0.1:  # —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π
                            chunk = np.pad(chunk, (0, int(sample_rate*0.1)-len(chunk))) if len(chunk) > 0 else np.zeros(int(sample_rate*0.1))
                        mfcc = librosa.feature.mfcc(y=chunk, sr=sample_rate, n_mfcc=13)
                        vec = np.mean(mfcc, axis=1)
                        feat_vectors.append(vec)
                    X = np.stack(feat_vectors, axis=0)
                    kmeans = KMeans(n_clusters=2, n_init=10, random_state=42)
                    labels = kmeans.fit_predict(X)
                    # –≠–≤—Ä–∏—Å—Ç–∏–∫–∞: –≤—ã–±–µ—Ä–µ–º –∫–ª–∞—Å—Ç–µ—Ä —Å —Å–µ–≥–º–µ–Ω—Ç–æ–º, —Å–æ–¥–µ—Ä–∂–∞—â–∏–º –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–µ, –∫–∞–∫ –æ–ø–µ—Ä–∞—Ç–æ—Ä
                    greeting_words = {"–∑–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ","–¥–æ–±—Ä—ã–π","–ø—Ä–∏–≤–µ—Ç","–æ–ø–µ—Ä–∞—Ç–æ—Ä","—á–µ–º","–º–æ–≥—É"}
                    cluster_scores = {0:0,1:0}
                    for lab, seg in zip(labels, segments):
                        tokens = set(seg['text'].lower().split())
                        if tokens & greeting_words:
                            cluster_scores[lab] += 1
                    if cluster_scores[0] == cluster_scores[1]:
                        # —Ç–∞–π–±—Ä–µ–π–∫: –∫–ª–∞—Å—Ç–µ—Ä —Å –±–æ–ª–µ–µ —Ä–∞–Ω–Ω–∏–º –ø–µ—Ä–≤—ã–º —Å–µ–≥–º–µ–Ω—Ç–æ–º -> –æ–ø–µ—Ä–∞—Ç–æ—Ä
                        first0 = min(seg['start'] for seg,l in zip(segments, labels) if l==0)
                        first1 = min(seg['start'] for seg,l in zip(segments, labels) if l==1)
                        operator_cluster = 0 if first0 <= first1 else 1
                    else:
                        operator_cluster = 0 if cluster_scores[0] > cluster_scores[1] else 1
                    # –ü—Ä–∏—Å–≤–∞–∏–≤–∞–µ–º —Ä–æ–ª–∏
                    for seg, lab in zip(segments, labels):
                        seg['speaker'] = 'operator' if lab == operator_cluster else 'client'
                    clustering_info = {
                        'method': 'kmeans-mfcc',
                        'operator_cluster': int(operator_cluster),
                        'cluster_scores': cluster_scores
                    }
                    print("‚úÖ –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
                except Exception as ce:
                    print(f"‚ö†Ô∏è –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –Ω–µ —É–¥–∞–ª–∞—Å—å: {ce}. –ò—Å–ø–æ–ª—å–∑—É—é —á–µ—Ä–µ–¥–æ–≤–∞–Ω–∏–µ.")
                    for idx, seg in enumerate(segments):
                        seg['speaker'] = 'client' if idx % 2 == 0 else 'operator'
                    clustering_info = {
                        'method': 'fallback-alternation',
                        'error': str(ce)[:120]
                    }
            else:
                # Fallback —á–µ—Ä–µ–¥–æ–≤–∞–Ω–∏–µ
                for idx, seg in enumerate(segments):
                    seg['speaker'] = 'client' if idx % 2 == 0 else 'operator'
                clustering_info = {
                    'method': 'alternation',
                    'note': 'Clustering off or dependencies missing'
                }

            flat_text = ' '.join(full_words_flat).strip()
            structured = {
                'text': flat_text,
                'segments': segments,
                'meta': {
                    'duration_sec': round(original_ms / 1000.0, 3),
                    'method': 'vosk',
                    'pause_threshold': pause_threshold,
                    'speaker_clustering': clustering_info
                }
            }
            print(f"‚úÖ Vosk –≥–æ—Ç–æ–≤–æ: {len(flat_text)} —Å–∏–º–≤–æ–ª–æ–≤, —Å–µ–≥–º–µ–Ω—Ç–æ–≤: {len(segments)}")
            return json.dumps(structured, ensure_ascii=False)
        except Exception as e:
            print(f"‚ö†Ô∏è Vosk –Ω–µ —É–¥–∞–ª–æ—Å—å: {e}. –ü–µ—Ä–µ—Ö–æ–∂—É –∫ Google SR")

    # --- Fallback Google ---
    try:
        recognizer = sr.Recognizer()
        temp_wav = "temp_transcribe.wav"
        audio_proc.export(temp_wav, format="wav")
        with sr.AudioFile(temp_wav) as source:
            recognizer.adjust_for_ambient_noise(source, duration=0.5)
            audio_data = recognizer.record(source)
            full_text = recognizer.recognize_google(audio_data, language='ru-RU')
        os.remove(temp_wav)
        full_text = full_text.strip()
        print(f"‚úÖ Google SR –≥–æ—Ç–æ–≤–æ: {len(full_text)} —Å–∏–º–≤–æ–ª–æ–≤")
        sentences = _split_sentences(full_text) or [full_text]
        total_chars = sum(len(s) for s in sentences) or 1
        current_start = 0.0
        segments = []
        for idx, sent in enumerate(sentences):
            proportion = len(sent) / total_chars
            seg_duration = proportion * (original_ms / 1000.0)
            start = current_start
            end = start + seg_duration
            speaker = 'client' if idx % 2 == 0 else 'operator'
            words = _approx_words(sent, start, end)
            gap_from_prev = 0.0 if idx == 0 else round(start - segments[-1]['end'], 3)
            segments.append({
                'index': idx,
                'speaker': speaker,
                'start': round(start, 3),
                'end': round(end, 3),
                'text': sent,
                'words': words,
                'gap_from_prev': gap_from_prev
            })
            current_start = end
        structured = {
            'text': full_text,
            'segments': segments,
            'meta': {
                'duration_sec': round(original_ms / 1000.0, 3),
                'method': 'google_speech + heuristic segmentation',
                'note': '–†–æ–ª–∏ —á–µ—Ä–µ–¥—É—é—Ç—Å—è; –ø–æ–¥–∫–ª—é—á–∏—Ç–µ VOSK_MODEL_PATH –¥–ª—è Vosk'
            }
        }
        return json.dumps(structured, ensure_ascii=False)
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ Google SR: {e}")
        return None


def transcribe_from_text(text: str):
    """–û–±–æ—Ä–∞—á–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –≤ —Ç—É –∂–µ JSON-—Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–ª—è —É–Ω–∏—Ñ–∏–∫–∞—Ü–∏–∏."""
    cleaned = text.strip()
    sentences = _split_sentences(cleaned) or [cleaned]
    total_chars = sum(len(s) for s in sentences) or 1
    # –ü—Ä–æ—Å—Ç–∞—è —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–∞—è —à–∫–∞–ª–∞ –≤—Ä–µ–º–µ–Ω–∏
    total_duration = len(cleaned.split()) * 0.4  # 0.4 —Å–µ–∫ –Ω–∞ —Å–ª–æ–≤–æ —ç–≤—Ä–∏—Å—Ç–∏–∫–∞
    current = 0.0
    segments = []
    for idx, sent in enumerate(sentences):
        proportion = len(sent) / total_chars
        seg_duration = proportion * total_duration
        start = current
        end = start + seg_duration
        words = _approx_words(sent, start, end)
        segments.append({
            'index': idx,
            'speaker': 'client' if idx % 2 == 0 else 'operator',
            'start': round(start, 3),
            'end': round(end, 3),
            'text': sent,
            'words': words,
            'gap_from_prev': 0.0 if idx == 0 else round(start - segments[-1]['end'], 3)
        })
        current = end
    structured = {
        'text': cleaned,
        'segments': segments,
        'meta': {
            'duration_sec': round(total_duration, 3),
            'method': 'text import + heuristic segmentation'
        }
    }
    return json.dumps(structured, ensure_ascii=False)