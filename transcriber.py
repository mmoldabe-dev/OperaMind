import os
import json
import math
import re
import speech_recognition as sr
from pydub import AudioSegment
from pathlib import Path
import concurrent.futures
from multiprocessing import cpu_count, Pool
import time

# Whisper
_WHISPER_AVAILABLE = False
try:
    from faster_whisper import WhisperModel
    _WHISPER_AVAILABLE = True
except Exception:
    pass

# Vosk
try:
    from vosk import Model as VoskModel, KaldiRecognizer
    _VOSK_AVAILABLE = True
except Exception:
    _VOSK_AVAILABLE = False

# –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è
try:
    import numpy as np
    import librosa
    from sklearn.cluster import KMeans
    _CLUSTER_AVAILABLE = True
except Exception:
    _CLUSTER_AVAILABLE = False


# ===================== –Ø–ó–´–ö–û–í–´–ï –ù–ê–°–¢–†–û–ô–ö–ò =====================

LANGUAGE_CONFIG = {
    'ru': {
        'name': '–†—É—Å—Å–∫–∏–π',
        'whisper_code': 'ru',
        'google_code': 'ru-RU',
        'vosk_model_env': 'VOSK_MODEL_PATH_RU',
        'greeting_words': {'–∑–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ', '–¥–æ–±—Ä—ã–π', '–ø—Ä–∏–≤–µ—Ç', '–æ–ø–µ—Ä–∞—Ç–æ—Ä', '—á–µ–º', '–º–æ–≥—É', '—Å–ª—É—à–∞—é', '–∞–ª–ª–æ'}
    },
    'kk': {
        'name': '“ö–∞–∑–∞“õ',
        'whisper_code': 'kk',
        'google_code': 'kk-KZ',
        'vosk_model_env': 'VOSK_MODEL_PATH_KK',
        'greeting_words': {'—Å”ô–ª–µ–º–µ—Ç—Å—ñ–∑', '—Å”ô–ª–µ–º', '“õ–∞–π—ã—Ä–ª—ã', '–∫“Ø–Ω', '–æ–ø–µ—Ä–∞—Ç–æ—Ä', '–∫”©–º–µ–∫', '—Ç—ã“£–¥–∞–π–º—ã–Ω', '–∞–ª–ª–æ'}
    }
}

def detect_language(text_sample):
    """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —è–∑—ã–∫–∞ –ø–æ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–Ω—ã–º –±—É–∫–≤–∞–º –∏ —Å–ª–æ–≤–∞–º"""
    if not text_sample:
        return os.getenv('DEFAULT_LANGUAGE', 'ru')
    
    sample_lower = text_sample.lower()
    
    # –ö–∞–∑–∞—Ö—Å–∫–∏–µ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –±—É–∫–≤—ã
    kk_specific = '”ô—ñ“£“ì“Ø“±“õ”©“ª'
    kk_char_count = sum(1 for c in sample_lower if c in kk_specific)
    
    # –°–∏–ª—å–Ω—ã–π –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä –∫–∞–∑–∞—Ö—Å–∫–æ–≥–æ
    if kk_char_count > 2:
        return 'kk'
    
    # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
    kk_keywords = {'—Å”ô–ª–µ–º', '“õ–∞–ª–∞–π', '–∂–∞“õ—Å—ã', '—Ä–∞—Ö–º–µ—Ç', '–∫–µ—à—ñ—Ä—ñ“£—ñ–∑', '”©—Ç—ñ–Ω–µ–º—ñ–Ω', '–∫”©–º–µ–∫—Ç–µ—Å—ñ“£—ñ–∑', '–±–∞—Ä', '–∂–æ“õ'}
    ru_keywords = {'–∑–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ', '–ø—Ä–∏–≤–µ—Ç', '–∫–∞–∫', '—Ö–æ—Ä–æ—à–æ', '—Å–ø–∞—Å–∏–±–æ', '–∏–∑–≤–∏–Ω–∏—Ç–µ', '–ø–æ–∂–∞–ª—É–π—Å—Ç–∞', '–ø–æ–º–æ–≥–∏—Ç–µ', '–µ—Å—Ç—å', '–Ω–µ—Ç'}
    
    kk_score = sum(1 for word in kk_keywords if word in sample_lower)
    ru_score = sum(1 for word in ru_keywords if word in sample_lower)
    
    if kk_score > ru_score:
        return 'kk'
    elif ru_score > kk_score:
        return 'ru'
    
    # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é —Ä—É—Å—Å–∫–∏–π
    return 'ru'


# ===================== –£–¢–ò–õ–ò–¢–´ =====================

def _split_sentences(text: str):
    raw = re.split(r'([.!?])', text)
    sentences = []
    current = ''
    for part in raw:
        if not part:
            continue
        current += part
        if part in '.!?':
            cleaned = current.strip()
            if cleaned:
                sentences.append(cleaned)
            current = ''
    tail = current.strip()
    if tail:
        sentences.append(tail)
    return [s for s in sentences if s]


def _approx_words(sentence_text: str, start_time: float, end_time: float):
    words = [w for w in re.split(r'\s+', sentence_text.strip()) if w]
    if not words:
        return []
    duration = max(end_time - start_time, 0.0001)
    per = duration / len(words)
    out = []
    t = start_time
    for w in words:
        out.append({'w': w, 'start': round(t, 3), 'end': round(t + per, 3)})
        t += per
    if out:
        out[-1]['end'] = round(end_time, 3)
    return out


# ===================== WHISPER (–ú–ê–ö–°–ò–ú–ê–õ–¨–ù–û–ï –ö–ê–ß–ï–°–¢–í–û) =====================

def transcribe_whisper(filepath: str, audio_ms: int):
    """Whisper —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º –∫–∞—á–µ—Å—Ç–≤–æ–º –±–µ–∑ –∫–æ–º–ø—Ä–æ–º–∏—Å—Å–æ–≤"""
    try:
        # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏
        model_name = os.getenv('WHISPER_MODEL', 'large-v3')
        device = os.getenv('WHISPER_DEVICE', 'cpu')
        
        # –î–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º float32, –Ω–µ int8
        compute = 'float32' if device == 'cpu' else 'float16'
        
        print(f"üü£ Whisper MAXIMUM QUALITY: model={model_name}, compute={compute}")
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º–∏ –ø–æ—Ç–æ–∫–∞–º–∏
        num_workers = cpu_count()
        model = WhisperModel(
            model_name, 
            device=device, 
            compute_type=compute,
            cpu_threads=num_workers,
            num_workers=num_workers
        )
        
        lang_mode = os.getenv('LANGUAGE_MODE', 'multi')
        
        print(f"   üí™ –ò—Å–ø–æ–ª—å–∑—É—é {num_workers} –ø–æ—Ç–æ–∫–æ–≤ CPU")
        
        # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∫–∞—á–µ—Å—Ç–≤–∞
        segments_iter, info = model.transcribe(
            str(filepath),
            language=None if lang_mode == 'multi' else LANGUAGE_CONFIG.get(lang_mode, {}).get('whisper_code', 'ru'),
            word_timestamps=True,
            beam_size=10,          # –£–≤–µ–ª–∏—á–µ–Ω–æ —Å 5 –¥–æ 10
            best_of=10,            # –£–≤–µ–ª–∏—á–µ–Ω–æ —Å 5 –¥–æ 10
            temperature=0.0,       # –î–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≤—ã–≤–æ–¥
            patience=2.0,          # –¢–µ—Ä–ø–µ–Ω–∏–µ –¥–ª—è beam search
            length_penalty=1.0,
            repetition_penalty=1.01,
            no_repeat_ngram_size=3,
            compression_ratio_threshold=2.4,
            log_prob_threshold=-1.0,
            no_speech_threshold=0.6,
            condition_on_previous_text=True,  # –ö–æ–Ω—Ç–µ–∫—Å—Ç –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —Ç–µ–∫—Å—Ç–∞
            initial_prompt="–≠—Ç–æ —Ä–∞–∑–≥–æ–≤–æ—Ä –æ–ø–µ—Ä–∞—Ç–æ—Ä–∞ call-—Ü–µ–Ω—Ç—Ä–∞ —Å –∫–ª–∏–µ–Ω—Ç–æ–º –Ω–∞ —Ä—É—Å—Å–∫–æ–º –∏–ª–∏ –∫–∞–∑–∞—Ö—Å–∫–æ–º —è–∑—ã–∫–µ.",
            vad_filter=True,       # Voice Activity Detection
            vad_parameters=dict(
                threshold=0.5,
                min_speech_duration_ms=250,
                max_speech_duration_s=float('inf'),
                min_silence_duration_ms=500,
                speech_pad_ms=400
            )
        )
        
        detected_lang = getattr(info, 'language', 'ru')
        lang_prob = getattr(info, 'language_probability', 0.0)
        print(f"   ‚úì –Ø–∑—ã–∫: {detected_lang} (–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å: {lang_prob:.2%})")
        
        segments = []
        all_words_flat = []
        idx = 0
        
        for seg in segments_iter:
            words = []
            for w in seg.words or []:
                word_text = w.word.strip()
                if word_text:  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ
                    words.append({
                        'w': word_text,
                        'start': round(w.start, 3),
                        'end': round(w.end, 3),
                        'probability': round(getattr(w, 'probability', 1.0), 3)
                    })
                    all_words_flat.append(word_text)
            
            text_clean = seg.text.strip()
            if not text_clean:
                continue
            
            segments.append({
                'index': idx,
                'speaker': 'unknown',
                'segment_type': 'speech',
                'start': round(seg.start, 3),
                'end': round(seg.end, 3),
                'text': text_clean,
                'words': words,
                'confidence': round(getattr(seg, 'avg_logprob', 0.0), 3),
                'gap_from_prev': 0.0 if idx == 0 else round(seg.start - segments[-1]['end'], 3)
            })
            idx += 1
        
        # –£–º–Ω–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–ø–∏–∫–µ—Ä–æ–≤ —Å –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–µ–π
        if _CLUSTER_AVAILABLE and len(segments) > 2:
            try:
                print("   üéØ –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —Å–ø–∏–∫–µ—Ä–æ–≤...")
                y, sr_audio = librosa.load(filepath, sr=16000, mono=True)
                
                features = []
                valid_segments = []
                
                for seg in segments:
                    start_sample = int(seg['start'] * sr_audio)
                    end_sample = int(seg['end'] * sr_audio)
                    
                    if end_sample > start_sample and end_sample <= len(y):
                        chunk = y[start_sample:end_sample]
                        
                        if len(chunk) > sr_audio * 0.1:  # –ú–∏–Ω–∏–º—É–º 0.1 —Å–µ–∫
                            # –ò–∑–≤–ª–µ–∫–∞–µ–º MFCC + –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
                            mfcc = librosa.feature.mfcc(y=chunk, sr=sr_audio, n_mfcc=20)
                            chroma = librosa.feature.chroma_stft(y=chunk, sr=sr_audio)
                            spectral = librosa.feature.spectral_centroid(y=chunk, sr=sr_audio)
                            
                            feature_vec = np.concatenate([
                                np.mean(mfcc, axis=1),
                                np.std(mfcc, axis=1),
                                np.mean(chroma, axis=1),
                                np.mean(spectral)
                            ])
                            
                            features.append(feature_vec)
                            valid_segments.append(seg)
                
                if len(features) >= 2:
                    X = np.vstack(features)
                    kmeans = KMeans(n_clusters=2, n_init=20, random_state=42, max_iter=500)
                    labels = kmeans.fit_predict(X)
                    
                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ–ø–µ—Ä–∞—Ç–æ—Ä–∞ –ø–æ –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏—è–º
                    lang_config = LANGUAGE_CONFIG.get(detected_lang, LANGUAGE_CONFIG['ru'])
                    cluster_scores = {0: 0, 1: 0}
                    cluster_first_time = {0: float('inf'), 1: float('inf')}
                    
                    for seg, label in zip(valid_segments, labels):
                        tokens = set(seg['text'].lower().split())
                        if tokens & lang_config['greeting_words']:
                            cluster_scores[label] += 2
                        cluster_first_time[label] = min(cluster_first_time[label], seg['start'])
                    
                    # –û–ø–µ—Ä–∞—Ç–æ—Ä: –±–æ–ª—å—à–µ –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–π –ò–õ–ò –≥–æ–≤–æ—Ä–∏—Ç –ø–µ—Ä–≤—ã–º
                    if cluster_scores[0] == cluster_scores[1]:
                        operator_cluster = 0 if cluster_first_time[0] < cluster_first_time[1] else 1
                    else:
                        operator_cluster = 0 if cluster_scores[0] > cluster_scores[1] else 1
                    
                    for seg, label in zip(valid_segments, labels):
                        seg['speaker'] = 'operator' if label == operator_cluster else 'client'
                    
                    print(f"   ‚úì –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è: –æ–ø–µ—Ä–∞—Ç–æ—Ä=–∫–ª–∞—Å—Ç–µ—Ä_{operator_cluster}")
            except Exception as e:
                print(f"   ‚ö†Ô∏è –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –Ω–µ —É–¥–∞–ª–∞—Å—å: {e}")
                # Fallback: —á–µ—Ä–µ–¥–æ–≤–∞–Ω–∏–µ
                for i, seg in enumerate(segments):
                    seg['speaker'] = 'client' if i % 2 == 0 else 'operator'
        else:
            # –ü—Ä–æ—Å—Ç–æ–µ —á–µ—Ä–µ–¥–æ–≤–∞–Ω–∏–µ
            for i, seg in enumerate(segments):
                seg['speaker'] = 'client' if i % 2 == 0 else 'operator'
        
        flat_text = ' '.join(all_words_flat).strip()
        
        return {
            'text': flat_text,
            'segments': segments,
            'meta': {
                'duration_sec': round(audio_ms / 1000.0, 3),
                'method': 'whisper_max_quality',
                'model': model_name,
                'language': detected_lang,
                'language_probability': lang_prob,
                'cpu_threads': num_workers
            }
        }
    except Exception as e:
        print(f"‚ö†Ô∏è Whisper –æ—à–∏–±–∫–∞: {e}")
        return None


# ===================== VOSK (–ö–ê–ß–ï–°–¢–í–û + –°–ö–û–†–û–°–¢–¨) =====================

def transcribe_vosk(filepath: str, audio_proc: AudioSegment, audio_ms: int, detected_lang: str = 'ru'):
    """Vosk —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é"""
    try:
        lang_config = LANGUAGE_CONFIG.get(detected_lang, LANGUAGE_CONFIG['ru'])
        model_path = os.getenv(lang_config['vosk_model_env'])
        
        if not model_path or not Path(model_path).exists():
            print(f"   ‚ö†Ô∏è Vosk –º–æ–¥–µ–ª—å –¥–ª—è {lang_config['name']} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {model_path}")
            return None
        
        print(f"üîä Vosk HIGH QUALITY: —è–∑—ã–∫={lang_config['name']}")
        
        temp_wav = "temp_vosk.wav"
        audio_proc.export(temp_wav, format="wav")
        
        from wave import open as wave_open
        wf = wave_open(temp_wav, "rb")
        
        model = VoskModel(model_path)
        rec = KaldiRecognizer(model, wf.getframerate())
        rec.SetWords(True)
        rec.SetMaxAlternatives(3)  # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏
        rec.SetPartialWords(True)
        
        results = []
        while True:
            data = wf.readframes(4000)
            if len(data) == 0:
                break
            if rec.AcceptWaveform(data):
                results.append(json.loads(rec.Result()))
        results.append(json.loads(rec.FinalResult()))
        wf.close()
        os.remove(temp_wav)
        
        # –°–æ–±–∏—Ä–∞–µ–º —Å–ª–æ–≤–∞ —Å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é
        raw_words = []
        for res in results:
            if isinstance(res, dict):
                for w in res.get('result', []) or []:
                    if 'word' in w:
                        raw_words.append({
                            'w': w.get('word'),
                            'start': float(w.get('start', 0.0)),
                            'end': float(w.get('end', 0.0)),
                            'conf': float(w.get('conf', 1.0))
                        })
        
        if not raw_words:
            raise RuntimeError("–ü—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç Vosk")
        
        # –ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –ø–æ –ø–∞—É–∑–∞–º
        pause_threshold = float(os.getenv('PAUSE_THRESHOLD', '0.6'))
        grouped = []
        current = [raw_words[0]]
        
        for prev, cur in zip(raw_words, raw_words[1:]):
            gap = cur['start'] - prev['end']
            # –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π –ø–æ—Ä–æ–≥: –¥–ª–∏–Ω–Ω—ã–µ –ø–∞—É–∑—ã = –Ω–æ–≤—ã–π —Å–µ–≥–º–µ–Ω—Ç
            adaptive_threshold = pause_threshold
            if gap > adaptive_threshold:
                grouped.append(current)
                current = [cur]
            else:
                current.append(cur)
        if current:
            grouped.append(current)
        
        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–µ–≥–º–µ–Ω—Ç–æ–≤
        segments = []
        full_words_flat = []
        
        for idx, g in enumerate(grouped):
            start_t = g[0]['start']
            end_t = g[-1]['end']
            text_seg = ' '.join(w['w'] for w in g)
            full_words_flat.extend(w['w'] for w in g)
            avg_conf = sum(w['conf'] for w in g) / len(g)
            
            segments.append({
                'index': idx,
                'speaker': 'unknown',
                'segment_type': 'speech',
                'start': round(start_t, 3),
                'end': round(end_t, 3),
                'text': text_seg,
                'words': [{'w': w['w'], 'start': round(w['start'], 3), 'end': round(w['end'], 3), 'confidence': round(w['conf'], 3)} for w in g],
                'confidence': round(avg_conf, 3),
                'gap_from_prev': 0.0 if idx == 0 else round(start_t - segments[-1]['end'], 3)
            })
        
        # –£–º–Ω–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–ø–∏–∫–µ—Ä–æ–≤
        for i, seg in enumerate(segments):
            tokens = set(seg['text'].lower().split())
            if tokens & lang_config['greeting_words']:
                seg['speaker'] = 'operator'
            else:
                seg['speaker'] = 'client' if i % 2 == 0 else 'operator'
        
        flat_text = ' '.join(full_words_flat).strip()
        
        return {
            'text': flat_text,
            'segments': segments,
            'meta': {
                'duration_sec': round(audio_ms / 1000.0, 3),
                'method': 'vosk_high_quality',
                'language': detected_lang,
                'model_path': model_path,
                'pause_threshold': pause_threshold
            }
        }
    except Exception as e:
        print(f"‚ö†Ô∏è Vosk –æ—à–∏–±–∫–∞: {e}")
        return None


# ===================== GOOGLE SR (–ê–ì–†–ï–°–°–ò–í–ù–ê–Ø –ü–ê–†–ê–õ–õ–ï–õ–ò–ó–ê–¶–ò–Ø) =====================

def transcribe_google_aggressive(filepath: str, audio_proc: AudioSegment, audio_ms: int, detected_lang: str = 'ru'):
    """Google SR —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–∞—Ü–∏–µ–π –∏ –¥–≤—É—è–∑—ã—á–Ω–æ–π –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π"""
    try:
        lang_config = LANGUAGE_CONFIG.get(detected_lang, LANGUAGE_CONFIG['ru'])
        google_lang = lang_config['google_code']
        
        # –ê–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è –Ω–∞—Ä–µ–∑–∫–∞ –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–∞
        chunk_seconds = float(os.getenv('GOOGLE_CHUNK_SECONDS', '6'))
        chunk_overlap = float(os.getenv('GOOGLE_CHUNK_OVERLAP', '3'))
        
        print(f"üîÅ Google SR AGGRESSIVE: —è–∑—ã–∫={lang_config['name']}")
        
        temp_wav = "temp_transcribe.wav"
        audio_proc.export(temp_wav, format="wav")
        
        from wave import open as wave_open
        wf = wave_open(temp_wav, 'rb')
        sample_rate = wf.getframerate()
        total_frames = wf.getnframes()
        total_duration = total_frames / sample_rate
        wf.close()
        
        def recognize_chunk_multilang(idx_chunk, ms_start, ms_end, start_pos, end_pos, primary_lang, secondary_lang):
            """–†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Å –ø–æ–ø—ã—Ç–∫–æ–π –¥–≤—É—Ö —è–∑—ã–∫–æ–≤ –¥–ª—è —Å–º–µ—à–∞–Ω–Ω—ã—Ö –¥–∏–∞–ª–æ–≥–æ–≤"""
            piece = audio_proc[ms_start:ms_end]
            piece_wav = f"temp_chunk_{idx_chunk}_{os.getpid()}.wav"
            piece.export(piece_wav, format='wav')
            
            local_recognizer = sr.Recognizer()
            with sr.AudioFile(piece_wav) as source:
                # –ê–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–∞
                local_recognizer.adjust_for_ambient_noise(source, duration=0.05)
                local_recognizer.energy_threshold = 200
                local_recognizer.dynamic_energy_threshold = False
                local_recognizer.pause_threshold = 0.2
                audio_data = local_recognizer.record(source)
                
                chunk_text = ''
                lang_used = primary_lang
                
                # –ü—Ä–æ–±—É–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π —è–∑—ã–∫
                try:
                    chunk_text = local_recognizer.recognize_google(audio_data, language=primary_lang)
                except sr.UnknownValueError:
                    # –ï—Å–ª–∏ –Ω–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–ª, –ø—Ä–æ–±—É–µ–º –≤—Ç–æ—Ä–æ–π —è–∑—ã–∫
                    try:
                        chunk_text = local_recognizer.recognize_google(audio_data, language=secondary_lang)
                        lang_used = secondary_lang
                    except Exception:
                        chunk_text = '[–Ω–µ—Ä–∞–∑–±–æ—Ä—á–∏–≤–æ]'
                except Exception:
                    chunk_text = ''
            
            try:
                os.remove(piece_wav)
            except:
                pass
            
            return {
                'idx': idx_chunk, 
                'start': start_pos, 
                'end': end_pos, 
                'text': chunk_text.strip(),
                'lang': lang_used
            }
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —á–∞–Ω–∫–æ–≤
        step = max(chunk_seconds - chunk_overlap, 1.0)
        chunk_jobs = []
        start_pos = 0.0
        idx_chunk = 0
        
        while start_pos < total_duration:
            end_pos = min(start_pos + chunk_seconds, total_duration)
            ms_start = int(start_pos * 1000)
            ms_end = int(end_pos * 1000)
            chunk_jobs.append((idx_chunk, ms_start, ms_end, start_pos, end_pos))
            if end_pos >= total_duration:
                break
            start_pos += step
            idx_chunk += 1
        
        # –ú–ê–ö–°–ò–ú–ê–õ–¨–ù–ê–Ø –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–∞—Ü–∏—è
        max_workers = min(cpu_count() * 2, len(chunk_jobs), 32)  # –í 2 —Ä–∞–∑–∞ –±–æ–ª—å—à–µ —è–¥–µ—Ä
        print(f"   üí™ –ê–ì–†–ï–°–°–ò–í–ù–ê–Ø –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–∞—Ü–∏—è: {max_workers} –ø–æ—Ç–æ–∫–æ–≤")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≤—Ç–æ—Ä–æ–π —è–∑—ã–∫ –¥–ª—è fallback
        secondary_lang = 'kk-KZ' if detected_lang == 'ru' else 'ru-RU'
        
        results = []
        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = {
                executor.submit(recognize_chunk_multilang, *job, google_lang, secondary_lang): job[0] 
                for job in chunk_jobs
            }
            for future in concurrent.futures.as_completed(futures):
                results.append(future.result())
        
        results.sort(key=lambda x: x['idx'])
        os.remove(temp_wav)
        
        # –£–º–Ω–∞—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è —Å —É—á—ë—Ç–æ–º —è–∑—ã–∫–∞
        raw_segments = []
        for res in results:
            if res['text'] and res['text'] != '[–Ω–µ—Ä–∞–∑–±–æ—Ä—á–∏–≤–æ]':
                raw_segments.append({
                    'start': res['start'], 
                    'end': res['end'], 
                    'text': res['text'],
                    'lang': res['lang']
                })
        
        if not raw_segments:
            raise RuntimeError('Google SR –Ω–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–ª —Ç–µ–∫—Å—Ç')
        
        deduped_segments = []
        if raw_segments:
            deduped_segments.append(raw_segments[0])
            for i in range(1, len(raw_segments)):
                prev_words = raw_segments[i-1]['text'].split()
                curr_words = raw_segments[i]['text'].split()
                
                # –£–º–Ω–∞—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è
                overlap_len = 0
                for j in range(1, min(len(prev_words), len(curr_words), 8) + 1):
                    if prev_words[-j:] == curr_words[:j]:
                        overlap_len = j
                
                dedupe_text = ' '.join(curr_words[overlap_len:]) if overlap_len > 0 else raw_segments[i]['text']
                if dedupe_text.strip():
                    deduped_segments.append({
                        'start': raw_segments[i]['start'],
                        'end': raw_segments[i]['end'],
                        'text': dedupe_text,
                        'lang': raw_segments[i]['lang']
                    })
        
        full_text = ' '.join(seg['text'] for seg in deduped_segments).strip()
        
        # –§–∏–Ω–∞–ª—å–Ω—ã–µ —Å–µ–≥–º–µ–Ω—Ç—ã —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º —Ç–∞–π–º–∏–Ω–≥–æ–º
        final_segments = []
        idx = 0
        
        for rs in deduped_segments:
            local_sents = _split_sentences(rs['text']) or [rs['text']]
            span = rs['end'] - rs['start']
            total_chars = sum(len(s) for s in local_sents) or 1
            cursor = rs['start']
            
            for s_text in local_sents:
                portion = len(s_text) / total_chars
                seg_duration = portion * span
                seg_start = cursor
                seg_end = seg_start + seg_duration
                words = _approx_words(s_text, seg_start, seg_end)
                
                final_segments.append({
                    'index': idx,
                    'speaker': 'client' if idx % 2 == 0 else 'operator',
                    'segment_type': 'speech',
                    'start': round(seg_start, 3),
                    'end': round(seg_end, 3),
                    'text': s_text,
                    'words': words,
                    'gap_from_prev': 0.0 if idx == 0 else round(seg_start - final_segments[-1]['end'], 3)
                })
                idx += 1
                cursor = seg_end
        
        return {
            'text': full_text,
            'segments': final_segments,
            'meta': {
                'duration_sec': round(audio_ms / 1000.0, 3),
                'method': 'google_speech_aggressive',
                'language': detected_lang,
                'workers': max_workers,
                'multilang': True
            }
        }
    except Exception as e:
        print(f"‚ùå Google SR –æ—à–∏–±–∫–∞: {e}")
        return None


# ===================== –ì–õ–ê–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø =====================

def transcribe_audio_file(filepath: str):
    """–ú–ê–ö–°–ò–ú–ê–õ–¨–ù–û–ï –ö–ê–ß–ï–°–¢–í–û –±–µ–∑ –∫–æ–º–ø—Ä–æ–º–∏—Å—Å–æ–≤"""
    print(f"\n{'='*70}")
    print(f"üì§ –¢–†–ê–ù–°–ö–†–ò–ü–¶–ò–Ø: {os.path.basename(filepath)}")
    print(f"{'='*70}")
    
    start_time = time.time()
    
    audio = AudioSegment.from_file(filepath)
    original_ms = len(audio)
    audio_proc = audio.set_frame_rate(16000).set_channels(1)
    duration_min = original_ms / 60000
    
    print(f"‚è±Ô∏è  –î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {duration_min:.1f} –º–∏–Ω")
    print(f"üíª –î–æ—Å—Ç—É–ø–Ω–æ CPU —è–¥–µ—Ä: {cpu_count()}")
    
    # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: Whisper > Vosk > Google
    enable_whisper = os.getenv('ENABLE_WHISPER', '1') == '1'
    enable_vosk = os.getenv('ENABLE_VOSK', '0') == '1'
    
    result = None
    
    # 1. WHISPER - –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å
    if enable_whisper and _WHISPER_AVAILABLE:
        result = transcribe_whisper(filepath, original_ms)
        if result:
            elapsed = time.time() - start_time
            print(f"\n‚úÖ –ì–û–¢–û–í–û –∑–∞ {elapsed:.1f}—Å (Whisper)")
            print(f"{'='*70}\n")
            return json.dumps(result, ensure_ascii=False)
    
    # 2. –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —è–∑—ã–∫–∞ –¥–ª—è fallback –º–µ—Ç–æ–¥–æ–≤
    sample_audio = audio_proc[:15000]  # –ü–µ—Ä–≤—ã–µ 15 —Å–µ–∫—É–Ω–¥
    sample_wav = "temp_lang_detect.wav"
    sample_audio.export(sample_wav, format="wav")
    
    detected_lang = 'ru'
    try:
        recognizer = sr.Recognizer()
        with sr.AudioFile(sample_wav) as source:
            recognizer.adjust_for_ambient_noise(source, duration=0.1)
            audio_data = recognizer.record(source)
            try:
                sample_text = recognizer.recognize_google(audio_data, language='ru-RU')
                detected_lang = detect_language(sample_text)
            except:
                # –ü—Ä–æ–±—É–µ–º –∫–∞–∑–∞—Ö—Å–∫–∏–π
                try:
                    sample_text = recognizer.recognize_google(audio_data, language='kk-KZ')
                    detected_lang = 'kk'
                except:
                    pass
        print(f"üîç –û–ø—Ä–µ–¥–µ–ª—ë–Ω —è–∑—ã–∫: {LANGUAGE_CONFIG[detected_lang]['name']}")
    except Exception as e:
        print(f"‚ö†Ô∏è –ê–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —è–∑—ã–∫–∞ –Ω–µ —É–¥–∞–ª–æ—Å—å: {e}")
    finally:
        if os.path.exists(sample_wav):
            os.remove(sample_wav)
    
    # 3. VOSK - –±—ã—Å—Ç—Ä–æ –∏ —Ç–æ—á–Ω–æ
    if enable_vosk and _VOSK_AVAILABLE and not result:
        result = transcribe_vosk(filepath, audio_proc, original_ms, detected_lang)
        if result:
            elapsed = time.time() - start_time
            print(f"\n‚úÖ –ì–û–¢–û–í–û –∑–∞ {elapsed:.1f}—Å (Vosk)")
            print(f"{'='*70}\n")
            return json.dumps(result, ensure_ascii=False)
    
    # 4. GOOGLE SR - –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–∞—Ü–∏—è
    if not result:
        result = transcribe_google_aggressive(filepath, audio_proc, original_ms, detected_lang)
        if result:
            elapsed = time.time() - start_time
            print(f"\n‚úÖ –ì–û–¢–û–í–û –∑–∞ {elapsed:.1f}—Å (Google)")
            print(f"{'='*70}\n")
            return json.dumps(result, ensure_ascii=False)
    
    raise RuntimeError("–í—Å–µ –º–µ—Ç–æ–¥—ã —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ –Ω–µ —É–¥–∞–ª–∏—Å—å")


def transcribe_from_text(text: str):
    """–ò–º–ø–æ—Ä—Ç —Ç–µ–∫—Å—Ç–∞ —Å –∞–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º —è–∑—ã–∫–∞"""
    cleaned = text.strip()
    detected_lang = detect_language(cleaned[:500])
    
    print(f"üìÑ –ò–º–ø–æ—Ä—Ç —Ç–µ–∫—Å—Ç–∞: —è–∑—ã–∫={LANGUAGE_CONFIG[detected_lang]['name']}")
    
    sentences = _split_sentences(cleaned) or [cleaned]
    total_chars = sum(len(s) for s in sentences) or 1
    total_duration = len(cleaned.split()) * 0.4
    current = 0.0
    segments = []
    
    for idx, sent in enumerate(sentences):
        proportion = len(sent) / total_chars
        seg_duration = proportion * total_duration
        start = current
        end = start + seg_duration
        words = _approx_words(sent, start, end)
        
        segments.append({
            'index': idx,
            'speaker': 'client' if idx % 2 == 0 else 'operator',
            'segment_type': 'speech',
            'start': round(start, 3),
            'end': round(end, 3),
            'text': sent,
            'words': words,
            'gap_from_prev': 0.0 if idx == 0 else round(start - segments[-1]['end'], 3)
        })
        current = end
    
    structured = {
        'text': cleaned,
        'segments': segments,
        'meta': {
            'duration_sec': round(total_duration, 3),
            'method': 'text_import',
            'language': detected_lang
        }
    }
    return json.dumps(structured, ensure_ascii=False)