import os
import json
import math
import re
import speech_recognition as sr
from pydub import AudioSegment
from pathlib import Path
import concurrent.futures

# –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ Whisper (faster-whisper)
_WHISPER_AVAILABLE = False
try:
    from faster_whisper import WhisperModel
    _WHISPER_AVAILABLE = True
except Exception:
    _WHISPER_AVAILABLE = False

try:
    from vosk import Model as VoskModel, KaldiRecognizer
    _VOSK_AVAILABLE = True
except Exception:
    _VOSK_AVAILABLE = False

# –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
try:
    import numpy as np
    import librosa
    from sklearn.cluster import KMeans
    _CLUSTER_AVAILABLE = True
except Exception:
    _CLUSTER_AVAILABLE = False


def _split_sentences(text: str):
    # –ü—Ä–æ—Å—Ç–µ–π—à–µ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
    raw = re.split(r'([.!?])', text)
    sentences = []
    current = ''
    for part in raw:
        if not part:
            continue
        current += part
        if part in '.!?':
            cleaned = current.strip()
            if cleaned:
                sentences.append(cleaned)
            current = ''
    # –•–≤–æ—Å—Ç
    tail = current.strip()
    if tail:
        sentences.append(tail)
    return [s for s in (seg.strip() for seg in sentences) if s]


def _approx_words(sentence_text: str, start_time: float, end_time: float):
    words = [w for w in re.split(r'\s+', sentence_text.strip()) if w]
    if not words:
        return []
    duration = max(end_time - start_time, 0.0001)
    per = duration / len(words)
    out = []
    t = start_time
    for w in words:
        out.append({
            'w': w,
            'start': round(t, 3),
            'end': round(t + per, 3)
        })
        t += per
    # –ö–æ—Ä—Ä–µ–∫—Ü–∏—è –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ
    if out:
        out[-1]['end'] = round(end_time, 3)
    return out


def transcribe_audio_file(filepath: str):
    """–†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è —Å –ø–æ–ø—ã—Ç–∫–æ–π Vosk.

    –ê–ª–≥–æ—Ä–∏—Ç–º:
            1. (–û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ) Whisper (ENABLE_WHISPER=1, –µ—Å–ª–∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω faster-whisper) –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —Ç–æ—á–Ω–æ–π —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏.
            2. –ó–∞—Ç–µ–º –ø—ã—Ç–∞–µ–º—Å—è Vosk (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω –∏ –µ—Å—Ç—å –º–æ–¥–µ–ª—å VOSK_MODEL_PATH).
            3. –ï—Å–ª–∏ –Ω–∏ Whisper –Ω–∏ Vosk ‚Äî fallback Google SpeechRecognition (chunked + —ç–≤—Ä–∏—Å—Ç–∏–∫–∞).
    """
    print(f"üì§ –ó–∞–≥—Ä—É–∂–∞—é: {filepath}")
    audio = AudioSegment.from_file(filepath)
    original_ms = len(audio)
    audio_proc = audio.set_frame_rate(16000).set_channels(1)
    duration_min = original_ms / 60000
    print(f"‚è±Ô∏è –î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {duration_min:.1f} –º–∏–Ω")

    # --- –ü–æ–ø—ã—Ç–∫–∞ Whisper (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ) ---
    enable_whisper = os.getenv('ENABLE_WHISPER', '0') == '1'
    whisper_model_name = os.getenv('WHISPER_MODEL', 'medium')
    if enable_whisper and _WHISPER_AVAILABLE:
        try:
            print(f"üü£ Whisper backend: –º–æ–¥–µ–ª—å={whisper_model_name}")
            # faster-whisper –æ—Ç–¥–∞—ë—Ç —É–∂–µ —Å–µ–≥–º–µ–Ω—Ç—ã —Å —Ç–∞–π–º–∫–æ–¥–∞–º–∏ –∏ —Å–ª–æ–≤–∞–º–∏ (–µ—Å–ª–∏ word_timestamps=True)
            model = WhisperModel(whisper_model_name, device=os.getenv('WHISPER_DEVICE','cpu'), compute_type=os.getenv('WHISPER_COMPUTE','int8'))
            segments_iter, info = model.transcribe(str(filepath), language='ru', word_timestamps=True)
            segments = []
            all_words_flat = []
            idx = 0
            for seg in segments_iter:
                words = []
                for w in seg.words or []:
                    words.append({
                        'w': w.word.strip(),
                        'start': round(w.start,3),
                        'end': round(w.end,3)
                    })
                    all_words_flat.append(w.word.strip())
                text_clean = seg.text.strip()
                if not text_clean:
                    continue
                segments.append({
                    'index': idx,
                    'speaker': 'unknown',
                    'segment_type': 'speech',
                    'start': round(seg.start,3),
                    'end': round(seg.end,3),
                    'text': text_clean,
                    'words': words,
                    'gap_from_prev': 0.0 if idx==0 else round(seg.start - segments[-1]['end'],3)
                })
                idx += 1
            # –ü—Ä–æ—Å—Ç–æ–µ —á–µ—Ä–µ–¥–æ–≤–∞–Ω–∏–µ —Ä–æ–ª–µ–π (–º–æ–∂–Ω–æ –ø–æ–∑–∂–µ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–æ–≤–∞—Ç—å ‚Äî reused logic —Å Vosk)
            for i, seg in enumerate(segments):
                seg['speaker'] = 'client' if i % 2 == 0 else 'operator'
            flat_text = ' '.join(all_words_flat).strip()
            structured = {
                'text': flat_text,
                'segments': segments,
                'meta': {
                    'duration_sec': round(original_ms / 1000.0, 3),
                    'method': 'whisper',
                    'model': whisper_model_name
                }
            }
            print(f"‚úÖ Whisper –≥–æ—Ç–æ–≤–æ: {len(flat_text)} —Å–∏–º–≤–æ–ª–æ–≤, —Å–µ–≥–º–µ–Ω—Ç–æ–≤: {len(segments)}")
            return json.dumps(structured, ensure_ascii=False)
        except Exception as we:
            print(f"‚ö†Ô∏è Whisper –Ω–µ —É–¥–∞–ª–æ—Å—å: {we}. –ü—Ä–æ–±—É—é Vosk/Google")

    # --- –ü–æ–ø—ã—Ç–∫–∞ Vosk ---
    model_path = os.getenv('VOSK_MODEL_PATH')
    if _VOSK_AVAILABLE and model_path and Path(model_path).exists():
        try:
            print("üîä –ò—Å–ø–æ–ª—å–∑—É—é Vosk –º–æ–¥–µ–ª—å")
            temp_wav = "temp_vosk.wav"
            audio_proc.export(temp_wav, format="wav")

            from wave import open as wave_open
            import wave, json as _json
            wf = wave_open(temp_wav, "rb")
            rec = KaldiRecognizer(VoskModel(model_path), wf.getframerate())
            rec.SetWords(True)

            import json as _json2
            results = []
            while True:
                data = wf.readframes(4000)
                if len(data) == 0:
                    break
                if rec.AcceptWaveform(data):
                    results.append(_json2.loads(rec.Result()))
            results.append(_json2.loads(rec.FinalResult()))
            wf.close()
            os.remove(temp_wav)

            # –°–æ–±–∏—Ä–∞–µ–º –í–°–ï —Å–ª–æ–≤–∞ –≤ –æ–¥–∏–Ω —Å–ø–∏—Å–æ–∫, –Ω–µ —Ç–µ—Ä—è—è filler words
            raw_words = []  # [{'word':, 'start':, 'end':}
            for res in results:
                if isinstance(res, dict):
                    for w in res.get('result', []) or []:
                        if 'word' in w:
                            raw_words.append({
                                'w': w.get('word'),
                                'start': float(w.get('start', 0.0)),
                                'end': float(w.get('end', 0.0))
                            })

            if not raw_words:
                raise RuntimeError("–ü—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç Vosk")

            # –°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –ø–æ –ø–∞—É–∑–∞–º > 0.8s + –≤—Å—Ç–∞–≤–∫–∞ —Å–æ–±—ã—Ç–∏–π, –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ FULL_AUDIO_EVENTS
            pause_threshold = float(os.getenv('PAUSE_THRESHOLD', '0.8'))
            full_audio_events = os.getenv('FULL_AUDIO_EVENTS', '0') == '1'
            grouped = []  # —Å–ø–∏—Å–æ–∫ —Å–µ–≥–º–µ–Ω—Ç–æ–≤ —Ä–µ—á–∏ (—Å–ª–æ–≤), events –±—É–¥–µ—Ç –ø–æ–∑–∂–µ
            current = [raw_words[0]]
            gaps = []  # [(gap_start, gap_end, gap_duration)]
            for prev, cur in zip(raw_words, raw_words[1:]):
                gap = cur['start'] - prev['end']
                if gap > pause_threshold:
                    grouped.append(current)
                    gaps.append((prev['end'], cur['start'], gap))
                    current = [cur]
                else:
                    current.append(cur)
            if current:
                grouped.append(current)

            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤ —Å–µ–≥–º–µ–Ω—Ç–æ–≤
            segments = []
            full_words_flat = []
            seg_counter = 0
            def add_speech_segment(g):
                nonlocal seg_counter
                start_t = g[0]['start']
                end_t = g[-1]['end']
                text_seg = ' '.join(w['w'] for w in g)
                full_words_flat.extend(w['w'] for w in g)
                segments.append({
                    'index': seg_counter,
                    'speaker': 'unknown',
                    'segment_type': 'speech',
                    'start': round(start_t, 3),
                    'end': round(end_t, 3),
                    'text': text_seg,
                    'words': [
                        {
                            'w': w['w'],
                            'start': round(w['start'], 3),
                            'end': round(w['end'], 3)
                        } for w in g
                    ],
                    'gap_from_prev': 0.0 if seg_counter == 0 else round(start_t - segments[-1]['end'], 3)
                })
                seg_counter += 1
            def add_gap_segment(gap_start, gap_end, gap_duration):
                nonlocal seg_counter
                if not full_audio_events:
                    return
                segments.append({
                    'index': seg_counter,
                    'speaker': 'none',
                    'segment_type': 'silence',
                    'start': round(gap_start, 3),
                    'end': round(gap_end, 3),
                    'text': f'[silence {gap_duration:.2f}s]',
                    'words': [],
                    'gap_from_prev': 0.0 if seg_counter == 0 else round(gap_start - segments[-1]['end'], 3)
                })
                seg_counter += 1

            # –ü–µ—Ä–µ–±–∏—Ä–∞–µ–º –≥—Ä—É–ø–ø—ã –∏ –º–µ–∂–≥—Ä—É–ø–ø–æ–≤—ã–µ –ø–∞—É–∑—ã
            for i, g in enumerate(grouped):
                add_speech_segment(g)
                if i < len(gaps):
                    gs, ge, gd = gaps[i]
                    add_gap_segment(gs, ge, gd)

            # –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —Å–ø–∏–∫–µ—Ä–æ–≤ (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ –∏ –¥–æ—Å—Ç—É–ø–Ω—ã –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏)
            clustering_enabled = os.getenv('SPEAKER_CLUSTERING', '1') == '1'
            clustering_info = {}
            if clustering_enabled and _CLUSTER_AVAILABLE:
                try:
                    print("üß™ –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —Å–ø–∏–∫–µ—Ä–æ–≤ (MFCC + KMeans)...")
                    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∞—É–¥–∏–æ –æ–¥–Ω–æ–π –≤–æ–ª–Ω–æ–π –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
                    # –í–ê–ñ–ù–û: sample_rate, —á—Ç–æ–±—ã –Ω–µ –ø–µ—Ä–µ–∫—Ä—ã—Ç—å –º–æ–¥—É–ª—å speech_recognition (sr)
                    y, sample_rate = librosa.load(filepath, sr=16000, mono=True)
                    feat_vectors = []
                    for seg in segments:
                        s = max(int(seg['start'] * sample_rate), 0)
                        e = min(int(seg['end'] * sample_rate), len(y))
                        chunk = y[s:e]
                        if len(chunk) < sample_rate * 0.1:  # —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π
                            chunk = np.pad(chunk, (0, int(sample_rate*0.1)-len(chunk))) if len(chunk) > 0 else np.zeros(int(sample_rate*0.1))
                        mfcc = librosa.feature.mfcc(y=chunk, sr=sample_rate, n_mfcc=13)
                        vec = np.mean(mfcc, axis=1)
                        feat_vectors.append(vec)
                    X = np.stack(feat_vectors, axis=0)
                    kmeans = KMeans(n_clusters=2, n_init=10, random_state=42)
                    labels = kmeans.fit_predict(X)
                    # –≠–≤—Ä–∏—Å—Ç–∏–∫–∞: –≤—ã–±–µ—Ä–µ–º –∫–ª–∞—Å—Ç–µ—Ä —Å —Å–µ–≥–º–µ–Ω—Ç–æ–º, —Å–æ–¥–µ—Ä–∂–∞—â–∏–º –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–µ, –∫–∞–∫ –æ–ø–µ—Ä–∞—Ç–æ—Ä
                    greeting_words = {"–∑–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ","–¥–æ–±—Ä—ã–π","–ø—Ä–∏–≤–µ—Ç","–æ–ø–µ—Ä–∞—Ç–æ—Ä","—á–µ–º","–º–æ–≥—É"}
                    cluster_scores = {0:0,1:0}
                    for lab, seg in zip(labels, segments):
                        tokens = set(seg['text'].lower().split())
                        if tokens & greeting_words:
                            cluster_scores[lab] += 1
                    if cluster_scores[0] == cluster_scores[1]:
                        # —Ç–∞–π–±—Ä–µ–π–∫: –∫–ª–∞—Å—Ç–µ—Ä —Å –±–æ–ª–µ–µ —Ä–∞–Ω–Ω–∏–º –ø–µ—Ä–≤—ã–º —Å–µ–≥–º–µ–Ω—Ç–æ–º -> –æ–ø–µ—Ä–∞—Ç–æ—Ä
                        first0 = min(seg['start'] for seg,l in zip(segments, labels) if l==0)
                        first1 = min(seg['start'] for seg,l in zip(segments, labels) if l==1)
                        operator_cluster = 0 if first0 <= first1 else 1
                    else:
                        operator_cluster = 0 if cluster_scores[0] > cluster_scores[1] else 1
                    # –ü—Ä–∏—Å–≤–∞–∏–≤–∞–µ–º —Ä–æ–ª–∏
                    for seg, lab in zip(segments, labels):
                        seg['speaker'] = 'operator' if lab == operator_cluster else 'client'
                    clustering_info = {
                        'method': 'kmeans-mfcc',
                        'operator_cluster': int(operator_cluster),
                        'cluster_scores': cluster_scores
                    }
                    print("‚úÖ –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
                except Exception as ce:
                    print(f"‚ö†Ô∏è –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –Ω–µ —É–¥–∞–ª–∞—Å—å: {ce}. –ò—Å–ø–æ–ª—å–∑—É—é —á–µ—Ä–µ–¥–æ–≤–∞–Ω–∏–µ.")
                    for idx, seg in enumerate(segments):
                        seg['speaker'] = 'client' if idx % 2 == 0 else 'operator'
                    clustering_info = {
                        'method': 'fallback-alternation',
                        'error': str(ce)[:120]
                    }
            else:
                # Fallback —á–µ—Ä–µ–¥–æ–≤–∞–Ω–∏–µ
                for idx, seg in enumerate(segments):
                    seg['speaker'] = 'client' if idx % 2 == 0 else 'operator'
                clustering_info = {
                    'method': 'alternation',
                    'note': 'Clustering off or dependencies missing'
                }

            flat_text = ' '.join(full_words_flat).strip()
            structured = {
                'text': flat_text,
                'segments': segments,
                'meta': {
                    'duration_sec': round(original_ms / 1000.0, 3),
                    'method': 'vosk',
                    'pause_threshold': pause_threshold,
                    'speaker_clustering': clustering_info
                }
            }
            print(f"‚úÖ Vosk –≥–æ—Ç–æ–≤–æ: {len(flat_text)} —Å–∏–º–≤–æ–ª–æ–≤, —Å–µ–≥–º–µ–Ω—Ç–æ–≤: {len(segments)}")
            return json.dumps(structured, ensure_ascii=False)
        except Exception as e:
            print(f"‚ö†Ô∏è Vosk –Ω–µ —É–¥–∞–ª–æ—Å—å: {e}. –ü–µ—Ä–µ—Ö–æ–∂—É –∫ Google SR")

    # --- Fallback Google (—Å –æ–ø—Ü–∏–µ–π chunked) ---
    try:
        enable_chunked = os.getenv('ENABLE_CHUNKED_GOOGLE', '1') == '1'
        chunk_seconds = float(os.getenv('GOOGLE_CHUNK_SECONDS', '10'))  # –£–º–µ–Ω—å—à–∏–ª –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞
        chunk_overlap = float(os.getenv('GOOGLE_CHUNK_OVERLAP', '3'))   # –£–≤–µ–ª–∏—á–∏–ª overlap
        full_audio_events = os.getenv('FULL_AUDIO_EVENTS', '0') == '1'
        detect_silence = os.getenv('DETECT_SILENCE_GOOGLE', '1') == '1'

        recognizer = sr.Recognizer()
        temp_wav = "temp_transcribe.wav"
        audio_proc.export(temp_wav, format="wav")

        from wave import open as wave_open
        wf = wave_open(temp_wav, 'rb')
        sample_rate = wf.getframerate()
        n_channels = wf.getnchannels()
        sampwidth = wf.getsampwidth()
        total_frames = wf.getnframes()
        total_duration = total_frames / sample_rate
        wf.close()

        words_master = []  # [{'w':text,'start':s,'end':e}]
        raw_segments = []  # –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–µ –∫—É—Å–∫–∏

        def recognize_chunk(idx_chunk, ms_start, ms_end, start_pos, end_pos):
            piece = audio_proc[ms_start:ms_end]
            piece_wav = f"temp_chunk_{idx_chunk}.wav"
            piece.export(piece_wav, format='wav')
            local_recognizer = sr.Recognizer()
            with sr.AudioFile(piece_wav) as source:
                local_recognizer.adjust_for_ambient_noise(source, duration=0.1)
                local_recognizer.energy_threshold = 300
                local_recognizer.dynamic_energy_threshold = False
                local_recognizer.pause_threshold = 0.3
                audio_data = local_recognizer.record(source)
                try:
                    chunk_text = local_recognizer.recognize_google(audio_data, language='ru-RU', show_all=False)
                except sr.RequestError:
                    try:
                        local_recognizer.energy_threshold = 500
                        local_recognizer.pause_threshold = 0.8
                        chunk_text = local_recognizer.recognize_google(audio_data, language='ru-RU')
                    except Exception:
                        chunk_text = ''
                except sr.UnknownValueError:
                    chunk_text = '[–Ω–µ—Ä–∞–∑–±–æ—Ä—á–∏–≤–æ]'
                except Exception:
                    chunk_text = ''
            os.remove(piece_wav)
            return {
                'idx': idx_chunk,
                'start': start_pos,
                'end': end_pos,
                'text': chunk_text.strip()
            }

        if enable_chunked and total_duration > chunk_seconds:
            print(f"üîÅ Chunked Google SR: total={total_duration:.1f}s chunk={chunk_seconds}s overlap={chunk_overlap}s")
            step = chunk_seconds - chunk_overlap
            if step <= 0:
                print("‚ö†Ô∏è –ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: OVERLAP >= CHUNK. –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É—é —à–∞–≥.")
                step = max(chunk_seconds * 0.8, 1.0)
            start_pos = 0.0
            idx_chunk = 0
            max_iter = int(math.ceil(total_duration / step) + 5)
            iter_count = 0
            chunk_jobs = []
            while start_pos < total_duration and iter_count < max_iter:
                end_pos = min(start_pos + chunk_seconds, total_duration)
                ms_start = int(start_pos * 1000)
                ms_end = int(end_pos * 1000)
                chunk_jobs.append((idx_chunk, ms_start, ms_end, start_pos, end_pos))
                if end_pos >= total_duration:
                    break
                start_pos += step
                idx_chunk += 1
                iter_count += 1
            if iter_count >= max_iter:
                print("‚ö†Ô∏è –î–æ—Å—Ç–∏–≥–Ω—É—Ç –ø—Ä–µ–¥–æ—Ö—Ä–∞–Ω–∏—Ç–µ–ª—å–Ω—ã–π –ª–∏–º–∏—Ç –∏—Ç–µ—Ä–∞—Ü–∏–π chunked —Ü–∏–∫–ª–∞ ‚Äî –≤–æ–∑–º–æ–∂–Ω–∞ –∞–Ω–æ–º–∞–ª–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.")
            # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —á–∞–Ω–∫–æ–≤
            results = []
            with concurrent.futures.ThreadPoolExecutor(max_workers=min(8, len(chunk_jobs))) as executor:
                future_to_idx = {executor.submit(recognize_chunk, *job): job[0] for job in chunk_jobs}
                for future in concurrent.futures.as_completed(future_to_idx):
                    res = future.result()
                    results.append(res)
            # –°–æ–±—Ä–∞—Ç—å –≤ –∏—Å—Ö–æ–¥–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ
            results.sort(key=lambda x: x['idx'])
            for res in results:
                if res['text']:
                    raw_segments.append({'start': res['start'], 'end': res['end'], 'text': res['text']})
        else:
            with sr.AudioFile(temp_wav) as source:
                # –ë–æ–ª–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–ª–æ–≤
                recognizer.adjust_for_ambient_noise(source, duration=0.3)
                recognizer.energy_threshold = 300
                recognizer.dynamic_energy_threshold = False
                recognizer.pause_threshold = 0.3
                audio_data = recognizer.record(source)
                whole_text = ''
                try:
                    whole_text = recognizer.recognize_google(audio_data, language='ru-RU', show_all=False)
                except Exception as ce:
                    print(f"‚ùå Google SR –æ—à–∏–±–∫–∞: {ce}")
                whole_text = whole_text.strip()
            if whole_text:
                raw_segments.append({'start': 0.0, 'end': total_duration, 'text': whole_text})

        os.remove(temp_wav)

        if not raw_segments:
            raise RuntimeError('Google SR –Ω–µ –≤–µ—Ä–Ω—É–ª —Ç–µ–∫—Å—Ç')

        # –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è overlap –º–µ–∂–¥—É chunk'–∞–º–∏
        deduped_segments = []
        if len(raw_segments) > 1:
            deduped_segments.append(raw_segments[0])
            for i in range(1, len(raw_segments)):
                prev_text = raw_segments[i-1]['text'].strip()
                curr_text = raw_segments[i]['text'].strip()
                
                # –ò—â–µ–º overlap –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —Å–ª–æ–≤ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ –∏ –ø–µ—Ä–≤—ã—Ö —Ç–µ–∫—É—â–µ–≥–æ
                prev_words = prev_text.split()
                curr_words = curr_text.split()
                
                overlap_len = 0
                for j in range(1, min(len(prev_words), len(curr_words)) + 1):
                    if prev_words[-j:] == curr_words[:j]:
                        overlap_len = j
                
                # –£–¥–∞–ª—è–µ–º overlap –∏–∑ —Ç–µ–∫—É—â–µ–≥–æ chunk'–∞
                if overlap_len > 0:
                    dedupe_curr = ' '.join(curr_words[overlap_len:])
                    print(f"üîÑ –£–¥–∞–ª–µ–Ω overlap {overlap_len} —Å–ª–æ–≤: {' '.join(curr_words[:overlap_len])}")
                else:
                    dedupe_curr = curr_text
                
                if dedupe_curr.strip():
                    deduped_segments.append({
                        'start': raw_segments[i]['start'],
                        'end': raw_segments[i]['end'], 
                        'text': dedupe_curr
                    })
        else:
            deduped_segments = raw_segments

        # –°–∫–ª–µ–π–∫–∞ —Ç–µ–∫—Å—Ç–∞ –ø–æ—Å–ª–µ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏
        full_text = ' '.join(seg['text'] for seg in deduped_segments).strip()

        # –†–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ —Å–µ–≥–º–µ–Ω—Ç—ã —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º –≤—Ä–µ–º–µ–Ω–∏
        final_segments = []
        idx = 0
        for rs in deduped_segments:
            # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –≤–Ω—É—Ç—Ä–∏ –∫—É—Å–æ—á–∫–∞ –¥–ª—è –ª—É—á—à–µ–≥–æ time mapping
            local_sents = _split_sentences(rs['text']) or [rs['text']]
            span = rs['end'] - rs['start']
            total_chars = sum(len(s) for s in local_sents) or 1
            cursor = rs['start']
            for s_text in local_sents:
                portion = len(s_text) / total_chars
                seg_duration = portion * span
                seg_start = cursor
                seg_end = seg_start + seg_duration
                words = _approx_words(s_text, seg_start, seg_end)
                final_segments.append({
                    'index': idx,
                    'speaker': 'unknown',
                    'segment_type': 'speech',
                    'start': round(seg_start,3),
                    'end': round(seg_end,3),
                    'text': s_text,
                    'words': words,
                    'gap_from_prev': 0.0 if idx==0 else round(seg_start - final_segments[-1]['end'],3)
                })
                idx += 1
                cursor = seg_end

        # –í—Å—Ç–∞–≤–∫–∞ —Å–µ–≥–º–µ–Ω—Ç–æ–≤ —Ç–∏—à–∏–Ω—ã –º–µ–∂–¥—É –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º–∏ –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ
        if full_audio_events and detect_silence:
            enriched = []
            for i, seg in enumerate(final_segments):
                enriched.append(seg)
                if i < len(final_segments)-1:
                    gap = final_segments[i+1]['start'] - seg['end']
                    if gap > 0.5:  # –ø–æ—Ä–æ–≥ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–ª—è fallback
                        enriched.append({
                            'index': idx,
                            'speaker': 'none',
                            'segment_type': 'silence',
                            'start': seg['end'],
                            'end': final_segments[i+1]['start'],
                            'text': f'[silence {gap:.2f}s]',
                            'words': [],
                            'gap_from_prev': round(seg['end'] - seg['end'],3)
                        })
                        idx += 1
            final_segments = enriched

        # –ü—Ä–æ—Å—Ç–æ–µ —á–µ—Ä–µ–¥–æ–≤–∞–Ω–∏–µ —Ä–æ–ª–µ–π (–∏–ª–∏ –æ—Å—Ç–∞–≤–∏—Ç—å unknown)
        for i, seg in enumerate(final_segments):
            if seg['speaker'] == 'unknown':
                seg['speaker'] = 'client' if i % 2 == 0 else 'operator'

        structured = {
            'text': full_text,
            'segments': final_segments,
            'meta': {
                'duration_sec': round(original_ms / 1000.0, 3),
                'method': 'google_speech_chunked' if enable_chunked else 'google_speech_single',
                'chunked': enable_chunked,
                'chunk_seconds': chunk_seconds,
                'overlap_seconds': chunk_overlap,
                'full_audio_events': full_audio_events
            }
        }
        return json.dumps(structured, ensure_ascii=False)
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ Google SR: {e}")
        return None


def transcribe_from_text(text: str):
    """–û–±–æ—Ä–∞—á–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –≤ —Ç—É –∂–µ JSON-—Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–ª—è —É–Ω–∏—Ñ–∏–∫–∞—Ü–∏–∏."""
    cleaned = text.strip()
    sentences = _split_sentences(cleaned) or [cleaned]
    total_chars = sum(len(s) for s in sentences) or 1
    # –ü—Ä–æ—Å—Ç–∞—è —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–∞—è —à–∫–∞–ª–∞ –≤—Ä–µ–º–µ–Ω–∏
    total_duration = len(cleaned.split()) * 0.4  # 0.4 —Å–µ–∫ –Ω–∞ —Å–ª–æ–≤–æ —ç–≤—Ä–∏—Å—Ç–∏–∫–∞
    current = 0.0
    segments = []
    for idx, sent in enumerate(sentences):
        proportion = len(sent) / total_chars
        seg_duration = proportion * total_duration
        start = current
        end = start + seg_duration
        words = _approx_words(sent, start, end)
        segments.append({
            'index': idx,
            'speaker': 'client' if idx % 2 == 0 else 'operator',
            'start': round(start, 3),
            'end': round(end, 3),
            'text': sent,
            'words': words,
            'gap_from_prev': 0.0 if idx == 0 else round(start - segments[-1]['end'], 3)
        })
        current = end
    structured = {
        'text': cleaned,
        'segments': segments,
        'meta': {
            'duration_sec': round(total_duration, 3),
            'method': 'text import + heuristic segmentation'
        }
    }
    return json.dumps(structured, ensure_ascii=False)